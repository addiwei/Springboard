{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 989 entries, 0 to 999\n",
      "Data columns (total 15 columns):\n",
      "search_word                  989 non-null object\n",
      "contentDetails.caption       989 non-null bool\n",
      "contentDetails.definition    989 non-null object\n",
      "catID                        989 non-null int64\n",
      "description                  975 non-null object\n",
      "localized.description        974 non-null object\n",
      "localized.title              989 non-null object\n",
      "tags                         930 non-null object\n",
      "title                        989 non-null object\n",
      "commentCount                 989 non-null float64\n",
      "dislikeCount                 989 non-null float64\n",
      "likeCount                    989 non-null float64\n",
      "view_bucket                  989 non-null float64\n",
      "age_int                      989 non-null int64\n",
      "target                       989 non-null int64\n",
      "dtypes: bool(1), float64(4), int64(3), object(7)\n",
      "memory usage: 116.9+ KB\n",
      "None\n",
      "(989, 15)\n",
      "  search_word  contentDetails.caption contentDetails.definition  catID  \\\n",
      "0     Siemens                   False                        hd     27   \n",
      "1     Siemens                   False                        hd     28   \n",
      "2     Siemens                   False                        hd     28   \n",
      "3     Siemens                   False                        hd     28   \n",
      "4     Siemens                   False                        hd     26   \n",
      "\n",
      "                                         description  \\\n",
      "0  15 Things You Didn't Know About SIEMENS | SUBS...   \n",
      "1  From digitalisation to automation, weƒ??re cha...   \n",
      "2  Are you interested in what Siemens does and wh...   \n",
      "3  The most powerful HVDC transformer in the worl...   \n",
      "4  Articolul complet pe site: https://cavaleria.r...   \n",
      "\n",
      "                               localized.description  \\\n",
      "0  15 Things You Didn't Know About SIEMENS | SUBS...   \n",
      "1  From digitalisation to automation, weƒ??re cha...   \n",
      "2  Are you interested in what Siemens does and wh...   \n",
      "3  The most powerful HVDC transformer in the worl...   \n",
      "4  Articolul complet pe site: https://cavaleria.r...   \n",
      "\n",
      "                                     localized.title  \\\n",
      "0            15 Things You Didn't Know About SIEMENS   \n",
      "1                What is it like to work at Siemens?   \n",
      "2                  Siemens - More than just business   \n",
      "3  Siemens presents: The first 1,100 kV HVDC Tran...   \n",
      "4  Ce ??nseamnŽŸ sŽŸ lucrezi la Siemens - Cavaler...   \n",
      "\n",
      "                                                tags  \\\n",
      "0  ['Alux', 'Alux.com', 'Alux Youtube', 'luxury l...   \n",
      "1  ['Siemens', 'Siemens UK', 'careers', 'jobs', '...   \n",
      "2  ['SIEMENS', 'products', 'Energy', 'Healthcare'...   \n",
      "3  ['Siemens', 'HVDC', 'Transformer', '1100kV', '...   \n",
      "4  ['Cavaleria.ro', 'hacking', 'siemens', 'job', ...   \n",
      "\n",
      "                                               title  commentCount  \\\n",
      "0            15 Things You Didn't Know About SIEMENS    117.000000   \n",
      "1                What is it like to work at Siemens?  11468.477674   \n",
      "2                  Siemens - More than just business  11468.477674   \n",
      "3  Siemens presents: The first 1,100 kV HVDC Tran...  11468.477674   \n",
      "4  Ce ??nseamnŽŸ sŽŸ lucrezi la Siemens - Cavaler...     64.000000   \n",
      "\n",
      "   dislikeCount  likeCount  view_bucket  age_int  target  \n",
      "0          80.0      931.0          1.0      323       0  \n",
      "1           7.0       70.0          1.0      372       0  \n",
      "2          52.0      731.0          2.0     1917       0  \n",
      "3         204.0     3835.0          1.0       42       2  \n",
      "4          23.0      718.0          1.0        0       0  \n"
     ]
    }
   ],
   "source": [
    "#Split and scale the data, train hyper parameters, Model, Evaluate\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "#from sklearn import cross_validation, metrics  \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "%store -r df\n",
    "\n",
    "print(df.info())\n",
    "print(df.shape)\n",
    "print(df.head())\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(989, 3)\n",
      "          0         1         2\n",
      "0  0.000190  0.000486  0.000166\n",
      "1  0.018638  0.000043  0.000012\n",
      "2  0.018638  0.000316  0.000130\n",
      "3  0.018638  0.001240  0.000682\n",
      "4  0.000104  0.000140  0.000128\n"
     ]
    }
   ],
   "source": [
    "#Scale all of the count data\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "minmax_scale = preprocessing.MinMaxScaler().fit(df[['commentCount','dislikeCount','likeCount']])\n",
    "\n",
    "X = pd.DataFrame(minmax_scale.transform(df[['commentCount','dislikeCount','likeCount']]))\n",
    "print(X.shape)\n",
    "y = df['target']\n",
    "\n",
    "\n",
    "n = pd.get_dummies(df[['search_word','contentDetails.definition','contentDetails.caption']])\n",
    "\n",
    "# X = pd.concat([X, n], axis=1)\n",
    "print(X.head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(989, 26)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the dummy columns as variables\n",
    "X = pd.DataFrame(np.hstack([X,n]))\n",
    "X.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(989, 27)\n"
     ]
    }
   ],
   "source": [
    "# Normalize the time series data and store as new date column\n",
    "ts = df['age_int']\n",
    "scaled_ts = (ts-ts.min())/(ts.max()-ts.min())\n",
    "\n",
    "X['Age'] = pd.Series(scaled_ts)\n",
    "X['Age'].fillna((X['Age'].mean()), inplace=True)\n",
    "\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000190146</td>\n",
       "      <td>0.000486296</td>\n",
       "      <td>0.000165565</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.083731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0186383</td>\n",
       "      <td>4.25509e-05</td>\n",
       "      <td>1.24485e-05</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.096052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0186383</td>\n",
       "      <td>0.000316092</td>\n",
       "      <td>0.000129998</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.484536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0186383</td>\n",
       "      <td>0.00124005</td>\n",
       "      <td>0.000681999</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000104011</td>\n",
       "      <td>0.00013981</td>\n",
       "      <td>0.000127686</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0            1            2      3  4  5  6  7  8  9    ...     \\\n",
       "0  0.000190146  0.000486296  0.000165565  False  1  0  0  0  0  0    ...      \n",
       "1    0.0186383  4.25509e-05  1.24485e-05  False  1  0  0  0  0  0    ...      \n",
       "2    0.0186383  0.000316092  0.000129998  False  1  0  0  0  0  0    ...      \n",
       "3    0.0186383   0.00124005  0.000681999  False  1  0  0  0  0  0    ...      \n",
       "4  0.000104011   0.00013981  0.000127686  False  1  0  0  0  0  0    ...      \n",
       "\n",
       "  17 18 19 20 21 22 23 24 25       Age  \n",
       "0  0  0  0  0  0  0  0  1  0  0.083731  \n",
       "1  0  0  0  0  0  0  0  1  0  0.096052  \n",
       "2  0  0  0  0  0  0  0  1  0  0.484536  \n",
       "3  0  0  0  0  0  0  0  1  0  0.013075  \n",
       "4  0  0  0  0  0  0  0  1  0  0.002514  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 890 entries, 304 to 684\n",
      "Data columns (total 27 columns):\n",
      "0      890 non-null object\n",
      "1      890 non-null object\n",
      "2      890 non-null object\n",
      "3      890 non-null object\n",
      "4      890 non-null object\n",
      "5      890 non-null object\n",
      "6      890 non-null object\n",
      "7      890 non-null object\n",
      "8      890 non-null object\n",
      "9      890 non-null object\n",
      "10     890 non-null object\n",
      "11     890 non-null object\n",
      "12     890 non-null object\n",
      "13     890 non-null object\n",
      "14     890 non-null object\n",
      "15     890 non-null object\n",
      "16     890 non-null object\n",
      "17     890 non-null object\n",
      "18     890 non-null object\n",
      "19     890 non-null object\n",
      "20     890 non-null object\n",
      "21     890 non-null object\n",
      "22     890 non-null object\n",
      "23     890 non-null object\n",
      "24     890 non-null object\n",
      "25     890 non-null object\n",
      "Age    890 non-null float64\n",
      "dtypes: float64(1), object(26)\n",
      "memory usage: 194.7+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.9, random_state=0)\n",
    "\n",
    "X_train.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.0931204\n",
      "1     0.118601\n",
      "2     0.175921\n",
      "3     0.108598\n",
      "4       0.0807\n",
      "5     0.104851\n",
      "6     0.121276\n",
      "7     0.084095\n",
      "8    0.0948788\n",
      "9     0.202743\n",
      "dtype: object\n",
      "<class 'numpy.ndarray'>\n",
      "Accuracy: 0.0\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.275\n",
      "Model:                            OLS   Adj. R-squared:                  0.254\n",
      "Method:                 Least Squares   F-statistic:                     13.12\n",
      "Date:                Sun, 03 Mar 2019   Prob (F-statistic):           2.80e-45\n",
      "Time:                        08:03:25   Log-Likelihood:                -1550.3\n",
      "No. Observations:                 890   AIC:                             3153.\n",
      "Df Residuals:                     864   BIC:                             3277.\n",
      "Df Model:                          25                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "0              1.3197      1.738      0.759      0.448      -2.092       4.732\n",
      "1              1.0567      1.307      0.809      0.419      -1.508       3.622\n",
      "2              1.3141      1.825      0.720      0.472      -2.268       4.896\n",
      "3              0.9717      0.142      6.860      0.000       0.694       1.250\n",
      "4             -1.2488      0.212     -5.877      0.000      -1.666      -0.832\n",
      "5              0.3813      0.210      1.815      0.070      -0.031       0.794\n",
      "6              0.2419      0.205      1.182      0.238      -0.160       0.644\n",
      "7             -0.6194      0.209     -2.962      0.003      -1.030      -0.209\n",
      "8              0.8443      0.239      3.527      0.000       0.375       1.314\n",
      "9              0.7024      0.201      3.490      0.001       0.307       1.097\n",
      "10            -0.4667      0.205     -2.279      0.023      -0.869      -0.065\n",
      "11             0.5354      0.210      2.554      0.011       0.124       0.947\n",
      "12             0.3275      0.210      1.561      0.119      -0.084       0.739\n",
      "13             0.3958      0.204      1.939      0.053      -0.005       0.796\n",
      "14            -0.1370      0.206     -0.666      0.506      -0.541       0.267\n",
      "15            -0.0350      0.206     -0.170      0.865      -0.439       0.369\n",
      "16             0.7561      0.205      3.687      0.000       0.354       1.159\n",
      "17             1.3866      0.202      6.864      0.000       0.990       1.783\n",
      "18             0.5602      0.270      2.078      0.038       0.031       1.089\n",
      "19            -0.1037      0.209     -0.497      0.620      -0.514       0.306\n",
      "20            -0.1824      0.200     -0.910      0.363      -0.576       0.211\n",
      "21             0.4833      0.214      2.262      0.024       0.064       0.903\n",
      "22            -0.3520      0.206     -1.712      0.087      -0.756       0.052\n",
      "23            -1.3153      0.217     -6.062      0.000      -1.741      -0.889\n",
      "24             1.3286      0.061     21.746      0.000       1.209       1.449\n",
      "25             0.8258      0.184      4.497      0.000       0.465       1.186\n",
      "Age            0.1464      0.384      0.381      0.703      -0.607       0.900\n",
      "==============================================================================\n",
      "Omnibus:                        9.633   Durbin-Watson:                   1.980\n",
      "Prob(Omnibus):                  0.008   Jarque-Bera (JB):                9.062\n",
      "Skew:                           0.205   Prob(JB):                       0.0108\n",
      "Kurtosis:                       2.725   Cond. No.                     2.25e+15\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 1.79e-28. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = sm.OLS(list(y_train), X_train.astype(float)).fit()\n",
    "y_pred = model.predict(X) \n",
    "# print(y_pred.head(10))\n",
    "# print(y_test.head(10))\n",
    "print(y_pred.head(10))\n",
    "print(type(y_test.values))\n",
    "#print(metrics.accuracy_score(y_test.values, y_pred.values))\n",
    "print('Accuracy: ' + str(np.mean(y_pred.values == y_test.values)))\n",
    "print_model = model.summary()\n",
    "print(print_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Lin Reg on test set: 0.07\n",
      "Linear Regression R squared: 0.0670\n",
      "Linear Regression RMSE: 1.5795\n",
      "Linear Regression MAE: 1.2996\n",
      "Accuracy of  train set: 0.0\n",
      "Accuracy of test set: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0xc930be0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "train_predicted = regressor.predict(X_train)\n",
    "\n",
    "\n",
    "print('Accuracy of Lin Reg on test set: {:.2f}'.format(regressor.score(X_test, y_test)))\n",
    "print('Linear Regression R squared: %.4f' % regressor.score(X_test, y_test))\n",
    "lin_mse = mean_squared_error(y_pred, y_test)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print('Linear Regression RMSE: %.4f' % lin_rmse)\n",
    "\n",
    "lin_mae = mean_absolute_error(y_pred, y_test)\n",
    "print('Linear Regression MAE: %.4f' % lin_mae)\n",
    "print('Accuracy of  train set: ' + str(np.mean(train_predicted == y_train)))\n",
    "print('Accuracy of test set: ' + str(np.mean(y_pred == y_test)))\n",
    "plt.scatter(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model is not a good fit for the data so move on to Log Reg.\n"
     ]
    }
   ],
   "source": [
    "print('Linear model is not a good fit for the data so move on to Log Reg.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "# set up classification report target variables\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = ['No Rank', 'White Belt', 'Blue Belt', 'Purple Belt', 'Brown Belt', 'Black Belt']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4444444444444444\n",
      "Logit R squared: 0.4444\n",
      "Logit RMSE: 1.8477\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.53      0.79      0.64        39\n",
      "  White Belt       0.57      0.36      0.44        22\n",
      "   Blue Belt       0.17      0.30      0.21        10\n",
      " Purple Belt       0.00      0.00      0.00         9\n",
      "  Brown Belt       0.00      0.00      0.00        14\n",
      "  Black Belt       0.33      0.40      0.36         5\n",
      "\n",
      "   micro avg       0.44      0.44      0.44        99\n",
      "   macro avg       0.27      0.31      0.28        99\n",
      "weighted avg       0.37      0.44      0.39        99\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "Logit = LogisticRegression()\n",
    "\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "#print(metrics.classification_report(y_test,y_pred))\n",
    "print('Logit R squared: %.4f' % clf.score(X_test, y_test))\n",
    "logit_mse = mean_squared_error(y_pred, y_test)\n",
    "logit_rmse = np.sqrt(logit_mse)\n",
    "print('Logit RMSE: %.4f' % logit_rmse)\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit relative poor performance without tuning. Next try some tuning\n"
     ]
    }
   ],
   "source": [
    "print('Logit relative poor performance without tuning. Next try some tuning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'C': 4641.588833612777}\n",
      "Accuracy is 0.37373737373737376\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.50      0.51      0.51        39\n",
      "  White Belt       0.42      0.50      0.46        22\n",
      "   Blue Belt       0.20      0.40      0.27        10\n",
      " Purple Belt       0.00      0.00      0.00         9\n",
      "  Brown Belt       0.00      0.00      0.00        14\n",
      "  Black Belt       0.25      0.40      0.31         5\n",
      "\n",
      "   micro avg       0.37      0.37      0.37        99\n",
      "   macro avg       0.23      0.30      0.26        99\n",
      "weighted avg       0.32      0.37      0.34        99\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "c_space = np.logspace(1, 5, 10)\n",
    "param_grid = {'C': c_space}\n",
    "\n",
    "logreg = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial', max_iter=200)\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "logreg_cv.fit(X_train,y_train)\n",
    "y_pred = logreg_cv.predict(X_test)\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "print(\"Accuracy is {}\".format(metrics.accuracy_score(y_test, y_pred)))\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even with tuning Log Reg is not good.\n"
     ]
    }
   ],
   "source": [
    "print('Even with tuning Log Reg is not good.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn:  1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.64      0.59      0.61        39\n",
      "  White Belt       0.48      0.45      0.47        22\n",
      "   Blue Belt       0.36      0.50      0.42        10\n",
      " Purple Belt       0.38      0.56      0.45         9\n",
      "  Brown Belt       0.57      0.29      0.38        14\n",
      "  Black Belt       0.38      0.60      0.46         5\n",
      "\n",
      "   micro avg       0.51      0.51      0.51        99\n",
      "   macro avg       0.47      0.50      0.47        99\n",
      "weighted avg       0.53      0.51      0.51        99\n",
      "\n",
      "Score:  0.5050505050505051\n",
      "\n",
      "nn:  2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.59      0.82      0.69        39\n",
      "  White Belt       0.44      0.32      0.37        22\n",
      "   Blue Belt       0.20      0.30      0.24        10\n",
      " Purple Belt       0.33      0.33      0.33         9\n",
      "  Brown Belt       0.50      0.07      0.12        14\n",
      "  Black Belt       0.67      0.40      0.50         5\n",
      "\n",
      "   micro avg       0.48      0.48      0.48        99\n",
      "   macro avg       0.46      0.37      0.38        99\n",
      "weighted avg       0.49      0.48      0.45        99\n",
      "\n",
      "Score:  0.48484848484848486\n",
      "\n",
      "nn:  3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.61      0.79      0.69        39\n",
      "  White Belt       0.56      0.45      0.50        22\n",
      "   Blue Belt       0.27      0.30      0.29        10\n",
      " Purple Belt       0.30      0.33      0.32         9\n",
      "  Brown Belt       0.60      0.21      0.32        14\n",
      "  Black Belt       0.50      0.40      0.44         5\n",
      "\n",
      "   micro avg       0.53      0.53      0.53        99\n",
      "   macro avg       0.47      0.42      0.43        99\n",
      "weighted avg       0.53      0.53      0.51        99\n",
      "\n",
      "Score:  0.5252525252525253\n",
      "\n",
      "nn:  4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.58      0.85      0.69        39\n",
      "  White Belt       0.54      0.32      0.40        22\n",
      "   Blue Belt       0.15      0.20      0.17        10\n",
      " Purple Belt       0.38      0.33      0.35         9\n",
      "  Brown Belt       0.60      0.21      0.32        14\n",
      "  Black Belt       0.67      0.40      0.50         5\n",
      "\n",
      "   micro avg       0.51      0.51      0.51        99\n",
      "   macro avg       0.49      0.39      0.41        99\n",
      "weighted avg       0.52      0.51      0.48        99\n",
      "\n",
      "Score:  0.5050505050505051\n",
      "\n",
      "nn:  5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.59      0.77      0.67        39\n",
      "  White Belt       0.50      0.36      0.42        22\n",
      "   Blue Belt       0.14      0.20      0.17        10\n",
      " Purple Belt       0.50      0.44      0.47         9\n",
      "  Brown Belt       0.71      0.36      0.48        14\n",
      "  Black Belt       0.67      0.40      0.50         5\n",
      "\n",
      "   micro avg       0.52      0.52      0.52        99\n",
      "   macro avg       0.52      0.42      0.45        99\n",
      "weighted avg       0.54      0.52      0.51        99\n",
      "\n",
      "Score:  0.5151515151515151\n",
      "\n",
      "nn:  6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.63      0.79      0.70        39\n",
      "  White Belt       0.59      0.45      0.51        22\n",
      "   Blue Belt       0.11      0.20      0.14        10\n",
      " Purple Belt       0.14      0.11      0.12         9\n",
      "  Brown Belt       0.75      0.21      0.33        14\n",
      "  Black Belt       0.50      0.40      0.44         5\n",
      "\n",
      "   micro avg       0.49      0.49      0.49        99\n",
      "   macro avg       0.45      0.36      0.38        99\n",
      "weighted avg       0.54      0.49      0.49        99\n",
      "\n",
      "Score:  0.494949494949495\n",
      "\n",
      "nn:  7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.63      0.79      0.70        39\n",
      "  White Belt       0.59      0.45      0.51        22\n",
      "   Blue Belt       0.19      0.30      0.23        10\n",
      " Purple Belt       0.11      0.11      0.11         9\n",
      "  Brown Belt       0.50      0.21      0.30        14\n",
      "  Black Belt       1.00      0.40      0.57         5\n",
      "\n",
      "   micro avg       0.51      0.51      0.51        99\n",
      "   macro avg       0.50      0.38      0.41        99\n",
      "weighted avg       0.53      0.51      0.50        99\n",
      "\n",
      "Score:  0.5050505050505051\n",
      "\n",
      "nn:  8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.64      0.77      0.70        39\n",
      "  White Belt       0.52      0.50      0.51        22\n",
      "   Blue Belt       0.11      0.20      0.14        10\n",
      " Purple Belt       0.00      0.00      0.00         9\n",
      "  Brown Belt       0.25      0.07      0.11        14\n",
      "  Black Belt       0.67      0.40      0.50         5\n",
      "\n",
      "   micro avg       0.46      0.46      0.46        99\n",
      "   macro avg       0.36      0.32      0.33        99\n",
      "weighted avg       0.45      0.46      0.44        99\n",
      "\n",
      "Score:  0.46464646464646464\n",
      "\n",
      "nn:  9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.59      0.74      0.66        39\n",
      "  White Belt       0.50      0.27      0.35        22\n",
      "   Blue Belt       0.12      0.30      0.17        10\n",
      " Purple Belt       0.00      0.00      0.00         9\n",
      "  Brown Belt       0.00      0.00      0.00        14\n",
      "  Black Belt       0.67      0.40      0.50         5\n",
      "\n",
      "   micro avg       0.40      0.40      0.40        99\n",
      "   macro avg       0.31      0.29      0.28        99\n",
      "weighted avg       0.39      0.40      0.38        99\n",
      "\n",
      "Score:  0.40404040404040403\n",
      "\n",
      "nn:  10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.60      0.82      0.70        39\n",
      "  White Belt       0.38      0.23      0.29        22\n",
      "   Blue Belt       0.00      0.00      0.00        10\n",
      " Purple Belt       0.00      0.00      0.00         9\n",
      "  Brown Belt       0.00      0.00      0.00        14\n",
      "  Black Belt       0.67      0.40      0.50         5\n",
      "\n",
      "   micro avg       0.39      0.39      0.39        99\n",
      "   macro avg       0.28      0.24      0.25        99\n",
      "weighted avg       0.36      0.39      0.36        99\n",
      "\n",
      "Score:  0.3939393939393939\n",
      "\n",
      "nn:  11\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.60      0.77      0.67        39\n",
      "  White Belt       0.56      0.45      0.50        22\n",
      "   Blue Belt       0.12      0.20      0.15        10\n",
      " Purple Belt       0.00      0.00      0.00         9\n",
      "  Brown Belt       0.00      0.00      0.00        14\n",
      "  Black Belt       0.67      0.40      0.50         5\n",
      "\n",
      "   micro avg       0.44      0.44      0.44        99\n",
      "   macro avg       0.32      0.30      0.30        99\n",
      "weighted avg       0.41      0.44      0.42        99\n",
      "\n",
      "Score:  0.4444444444444444\n",
      "\n",
      "nn:  12\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.61      0.85      0.71        39\n",
      "  White Belt       0.56      0.41      0.47        22\n",
      "   Blue Belt       0.17      0.30      0.21        10\n",
      " Purple Belt       0.00      0.00      0.00         9\n",
      "  Brown Belt       0.00      0.00      0.00        14\n",
      "  Black Belt       0.67      0.40      0.50         5\n",
      "\n",
      "   micro avg       0.47      0.47      0.47        99\n",
      "   macro avg       0.33      0.33      0.32        99\n",
      "weighted avg       0.42      0.47      0.43        99\n",
      "\n",
      "Score:  0.47474747474747475\n",
      "\n",
      "nn:  13\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.60      0.82      0.70        39\n",
      "  White Belt       0.50      0.45      0.48        22\n",
      "   Blue Belt       0.12      0.20      0.15        10\n",
      " Purple Belt       0.25      0.11      0.15         9\n",
      "  Brown Belt       0.00      0.00      0.00        14\n",
      "  Black Belt       0.67      0.40      0.50         5\n",
      "\n",
      "   micro avg       0.47      0.47      0.47        99\n",
      "   macro avg       0.36      0.33      0.33        99\n",
      "weighted avg       0.42      0.47      0.43        99\n",
      "\n",
      "Score:  0.47474747474747475\n",
      "\n",
      "nn:  14\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.61      0.87      0.72        39\n",
      "  White Belt       0.53      0.45      0.49        22\n",
      "   Blue Belt       0.13      0.20      0.16        10\n",
      " Purple Belt       0.33      0.11      0.17         9\n",
      "  Brown Belt       0.25      0.07      0.11        14\n",
      "  Black Belt       0.50      0.20      0.29         5\n",
      "\n",
      "   micro avg       0.49      0.49      0.49        99\n",
      "   macro avg       0.39      0.32      0.32        99\n",
      "weighted avg       0.46      0.49      0.45        99\n",
      "\n",
      "Score:  0.494949494949495\n",
      "\n",
      "nn:  15\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.62      0.85      0.72        39\n",
      "  White Belt       0.48      0.50      0.49        22\n",
      "   Blue Belt       0.07      0.10      0.08        10\n",
      " Purple Belt       0.00      0.00      0.00         9\n",
      "  Brown Belt       0.25      0.07      0.11        14\n",
      "  Black Belt       0.50      0.20      0.29         5\n",
      "\n",
      "   micro avg       0.47      0.47      0.47        99\n",
      "   macro avg       0.32      0.29      0.28        99\n",
      "weighted avg       0.42      0.47      0.43        99\n",
      "\n",
      "Score:  0.47474747474747475\n",
      "\n",
      "nn:  16\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.59      0.85      0.69        39\n",
      "  White Belt       0.41      0.41      0.41        22\n",
      "   Blue Belt       0.07      0.10      0.08        10\n",
      " Purple Belt       0.00      0.00      0.00         9\n",
      "  Brown Belt       0.00      0.00      0.00        14\n",
      "  Black Belt       0.50      0.20      0.29         5\n",
      "\n",
      "   micro avg       0.44      0.44      0.44        99\n",
      "   macro avg       0.26      0.26      0.25        99\n",
      "weighted avg       0.36      0.44      0.39        99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score:  0.4444444444444444\n",
      "\n",
      "nn:  17\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.56      0.87      0.68        39\n",
      "  White Belt       0.50      0.36      0.42        22\n",
      "   Blue Belt       0.12      0.20      0.15        10\n",
      " Purple Belt       0.00      0.00      0.00         9\n",
      "  Brown Belt       0.00      0.00      0.00        14\n",
      "  Black Belt       1.00      0.20      0.33         5\n",
      "\n",
      "   micro avg       0.45      0.45      0.45        99\n",
      "   macro avg       0.36      0.27      0.26        99\n",
      "weighted avg       0.39      0.45      0.39        99\n",
      "\n",
      "Score:  0.45454545454545453\n",
      "\n",
      "nn:  18\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.55      0.85      0.67        39\n",
      "  White Belt       0.44      0.36      0.40        22\n",
      "   Blue Belt       0.12      0.20      0.15        10\n",
      " Purple Belt       0.00      0.00      0.00         9\n",
      "  Brown Belt       0.00      0.00      0.00        14\n",
      "  Black Belt       1.00      0.20      0.33         5\n",
      "\n",
      "   micro avg       0.44      0.44      0.44        99\n",
      "   macro avg       0.35      0.27      0.26        99\n",
      "weighted avg       0.38      0.44      0.38        99\n",
      "\n",
      "Score:  0.4444444444444444\n",
      "\n",
      "nn:  19\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.58      0.90      0.71        39\n",
      "  White Belt       0.45      0.41      0.43        22\n",
      "   Blue Belt       0.21      0.30      0.25        10\n",
      " Purple Belt       0.00      0.00      0.00         9\n",
      "  Brown Belt       0.00      0.00      0.00        14\n",
      "  Black Belt       1.00      0.20      0.33         5\n",
      "\n",
      "   micro avg       0.48      0.48      0.48        99\n",
      "   macro avg       0.37      0.30      0.29        99\n",
      "weighted avg       0.40      0.48      0.42        99\n",
      "\n",
      "Score:  0.48484848484848486\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "nns = np.arange(1,20,1)\n",
    "\n",
    "def train_and_predict(nn):\n",
    "    knn = Pipeline([\n",
    "                        ('knn', KNeighborsClassifier(n_neighbors=nn) )\n",
    "                        ])\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    score = metrics.accuracy_score(y_test, y_pred)\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "for nn in nns:\n",
    "    print('nn: ', nn)\n",
    "    print('Score: ', train_and_predict(nn))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42424242424242425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.46      0.92      0.61        39\n",
      "  White Belt       0.40      0.09      0.15        22\n",
      "   Blue Belt       0.15      0.20      0.17        10\n",
      " Purple Belt       0.00      0.00      0.00         9\n",
      "  Brown Belt       0.00      0.00      0.00        14\n",
      "  Black Belt       1.00      0.40      0.57         5\n",
      "\n",
      "   micro avg       0.42      0.42      0.42        99\n",
      "   macro avg       0.33      0.27      0.25        99\n",
      "weighted avg       0.33      0.42      0.32        99\n",
      "\n",
      "SVM R squared: 0.4242\n",
      "SVM RMSE: 1.8477\n",
      "0.31971861334016416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Try SVM\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "clf = svm.SVC(gamma='scale', random_state=0)\n",
    "clf.fit(X_train, y_train) \n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "print('SVM R squared: %.4f' % clf.score(X_test, y_test))\n",
    "SVM_mse = mean_squared_error(y_pred, y_test)\n",
    "SVM_rmse = np.sqrt(SVM_mse)\n",
    "print('SVM RMSE: %.4f' % logit_rmse)\n",
    "\n",
    "print(f1_score(y_true=y_test, y_pred=y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.72      0.67      0.69        39\n",
      "  White Belt       0.81      0.77      0.79        22\n",
      "   Blue Belt       0.45      0.50      0.48        10\n",
      " Purple Belt       0.45      0.56      0.50         9\n",
      "  Brown Belt       0.70      0.50      0.58        14\n",
      "  Black Belt       0.30      0.60      0.40         5\n",
      "\n",
      "   micro avg       0.64      0.64      0.64        99\n",
      "   macro avg       0.57      0.60      0.57        99\n",
      "weighted avg       0.67      0.64      0.65        99\n",
      "\n",
      "0.6363636363636364\n",
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 6, 'min_samples_leaf': 1}\n",
      "Best score is 0.5449438202247191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Try Decision Tree Classifier\n",
    "\n",
    "from scipy.stats import randint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": randint(1, 9),\n",
    "              \"min_samples_leaf\": randint(1, 9),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Instantiate the RandomizedSearchCV object: tree_cv\n",
    "tree_cv = RandomizedSearchCV(tree, param_dist, cv=5, random_state=0)\n",
    "\n",
    "# Fit it to the data\n",
    "tree_cv.fit(X_train,y_train)\n",
    "\n",
    "y_pred = tree_cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "# Print the tuned parameters and score\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\n",
    "print(\"Best score is {}\".format(tree_cv.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try random forest with default hyperparameters\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_class = RandomForestClassifier(random_state=42)\n",
    "forest_class.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest R squared: 0.6869\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.74      0.82      0.78        39\n",
      "  White Belt       0.84      0.73      0.78        22\n",
      "   Blue Belt       0.35      0.60      0.44        10\n",
      " Purple Belt       0.71      0.56      0.63         9\n",
      "  Brown Belt       0.75      0.43      0.55        14\n",
      "  Black Belt       0.60      0.60      0.60         5\n",
      "\n",
      "   micro avg       0.69      0.69      0.69        99\n",
      "   macro avg       0.67      0.62      0.63        99\n",
      "weighted avg       0.72      0.69      0.69        99\n",
      "\n",
      "Random Forest RMSE: 1.3143\n",
      "0.6868686868686869\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest R squared: %.4f' % forest_class.score(X_test, y_test))\n",
    "y_pred = forest_class.predict(X_test)\n",
    "forest_mse = mean_squared_error(y_pred, y_test)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "print('Random Forest RMSE: %.4f' % forest_rmse)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and select optimal hyperparameters\n",
    "print(forest_class.get_params())\n",
    "\n",
    "#Create a random grid for possible parameters to attempt and then use random search\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   14.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   58.8s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  2.0min finished\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.79      0.77      0.78        39\n",
      "  White Belt       0.85      0.77      0.81        22\n",
      "   Blue Belt       0.53      0.80      0.64        10\n",
      " Purple Belt       0.54      0.78      0.64         9\n",
      "  Brown Belt       0.88      0.50      0.64        14\n",
      "  Black Belt       0.40      0.40      0.40         5\n",
      "\n",
      "   micro avg       0.72      0.72      0.72        99\n",
      "   macro avg       0.66      0.67      0.65        99\n",
      "weighted avg       0.75      0.72      0.72        99\n",
      "\n",
      "0.7171717171717171\n"
     ]
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, scoring = 'neg_mean_squared_error', n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n",
    "y_pred = rf_random.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "# Print the tuned parameters and score\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 600, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 60, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "#Print and evaluate best parameters from the search\n",
    "print(rf_random.best_params_)\n",
    "\n",
    "best_random = rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate performance of the base model w/ searched hyperparameters\n",
    "\n",
    "random_model = RandomForestClassifier(n_estimators = 1155, min_samples_split=5, min_samples_leaf=2, \n",
    "                                   max_features = 'sqrt', max_depth = 10, bootstrap = False, random_state = 42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=10, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=2, min_samples_split=5,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1155, n_jobs=None,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.79      0.77      0.78        39\n",
      "  White Belt       0.85      0.77      0.81        22\n",
      "   Blue Belt       0.53      0.80      0.64        10\n",
      " Purple Belt       0.54      0.78      0.64         9\n",
      "  Brown Belt       0.88      0.50      0.64        14\n",
      "  Black Belt       0.40      0.40      0.40         5\n",
      "\n",
      "   micro avg       0.72      0.72      0.72        99\n",
      "   macro avg       0.66      0.67      0.65        99\n",
      "weighted avg       0.75      0.72      0.72        99\n",
      "\n",
      "0.7171717171717171\n",
      "Random Forest R squared: 0.6667\n",
      "Random Forest RMSE: 1.3181\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "#Rerun of random forest with random searched hyperparameters- see results are the same as previous\n",
    "y_pred = rf_random.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "# Print the tuned parameters and score\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print('Random Forest R squared: %.4f' % random_model.score(X_test, y_test))\n",
    "y_pred = random_model.predict(X_test)\n",
    "forest_mse = mean_squared_error(y_pred, y_test)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "print('Random Forest RMSE: %.4f' % forest_rmse)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Grid Search to see if we can improve the hyperparameters further. \n",
    "# Setting grid around the previously identified optimal values\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [30, 40, 50, 60, 70],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_samples_split': [2, 3, 4, 5],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, scoring = 'neg_mean_squared_error',\n",
    "                          cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 480 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done 371 tasks      | elapsed:   44.6s\n",
      "[Parallel(n_jobs=-1)]: Done 654 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1019 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  3.0min finished\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 60,\n",
       " 'max_features': 3,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.78      0.79      0.78        39\n",
      "  White Belt       0.83      0.68      0.75        22\n",
      "   Blue Belt       0.50      0.80      0.62        10\n",
      " Purple Belt       0.64      0.78      0.70         9\n",
      "  Brown Belt       0.75      0.43      0.55        14\n",
      "  Black Belt       0.33      0.40      0.36         5\n",
      "\n",
      "   micro avg       0.70      0.70      0.70        99\n",
      "   macro avg       0.64      0.65      0.63        99\n",
      "weighted avg       0.72      0.70      0.70        99\n",
      "\n",
      "0.696969696969697\n",
      "Random Forest R squared: -1.7273\n",
      "Random Forest RMSE: 1.3143\n",
      "0.696969696969697\n"
     ]
    }
   ],
   "source": [
    "#Evaluate performance of random forest with grid search hyperparameters\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "# Print the tuned parameters and score\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print('Random Forest R squared: %.4f' % grid_search.score(X_test, y_test))\n",
    "y_pred = grid_search.predict(X_test)\n",
    "forest_mse = mean_squared_error(y_pred, y_test)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "print('Random Forest RMSE: %.4f' % forest_rmse)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6464646464646465\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.71      0.64      0.68        39\n",
      "  White Belt       0.78      0.64      0.70        22\n",
      "   Blue Belt       0.50      0.80      0.62        10\n",
      " Purple Belt       0.58      0.78      0.67         9\n",
      "  Brown Belt       0.89      0.57      0.70        14\n",
      "  Black Belt       0.22      0.40      0.29         5\n",
      "\n",
      "   micro avg       0.65      0.65      0.65        99\n",
      "   macro avg       0.61      0.64      0.61        99\n",
      "weighted avg       0.69      0.65      0.66        99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try Gradient Boosting\n",
    "model = ensemble.GradientBoostingClassifier(random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting R squared: 0.6465\n",
      "Gradient Boosting RMSE: 1.7203\n"
     ]
    }
   ],
   "source": [
    "print('Gradient Boosting R squared: %.4f' % model.score(X_test, y_test))\n",
    "y_pred = model.predict(X_test)\n",
    "model_mse = mean_squared_error(y_pred, y_test)\n",
    "model_rmse = np.sqrt(model_mse)\n",
    "print('Gradient Boosting RMSE: %.4f' % model_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'deviance', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'presort': 'auto', 'random_state': 0, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print(model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [100, 311, 522, 733, 944, 1155, 1366, 1577, 1788, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [2, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(2, 10, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   29.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  4.6min finished\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_sampl...      subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [100, 311, 522, 733, 944, 1155, 1366, 1577, 1788, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [2, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring='neg_mean_squared_error',\n",
       "          verbose=2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Basic GBM performance was poor - try tuning learning rate and number of trees\n",
    "gbm = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "gbm_random = RandomizedSearchCV(estimator = gbm, param_distributions = random_grid, scoring = 'neg_mean_squared_error', n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "gbm_random.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1577, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 6}\n"
     ]
    }
   ],
   "source": [
    "#Print and evaluate best parameters from the search\n",
    "print(gbm_random.best_params_)\n",
    "\n",
    "best_random = gbm_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate performance of the base model w/ searched hyperparameters\n",
    "\n",
    "random_model = GradientBoostingClassifier(n_estimators = 1577, min_samples_split=2, min_samples_leaf=4, \n",
    "                                   max_features = 'sqrt', max_depth = 6, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=6,\n",
       "              max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=4, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=1577,\n",
       "              n_iter_no_change=None, presort='auto', random_state=42,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM - random searched R squared: 0.6566\n",
      "0.6565656565656566\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.69      0.69      0.69        39\n",
      "  White Belt       0.78      0.64      0.70        22\n",
      "   Blue Belt       0.53      0.80      0.64        10\n",
      " Purple Belt       0.58      0.78      0.67         9\n",
      "  Brown Belt       0.86      0.43      0.57        14\n",
      "  Black Belt       0.38      0.60      0.46         5\n",
      "\n",
      "   micro avg       0.66      0.66      0.66        99\n",
      "   macro avg       0.64      0.66      0.62        99\n",
      "weighted avg       0.69      0.66      0.66        99\n",
      "\n",
      "GBM - random searched RMSE: 1.3143\n"
     ]
    }
   ],
   "source": [
    "#Evaluate performance of GBM with random searched hyperparameters\n",
    "print('GBM - random searched R squared: %.4f' % random_model.score(X_test, y_test))\n",
    "y_pred = random_model.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "GBM_mse = mean_squared_error(y_pred, y_test)\n",
    "GBM_rmse = np.sqrt(GBM_mse)\n",
    "print('GBM - random searched RMSE: %.4f' % forest_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GBM with random searched hyperparameters on par with Random Forest'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('GBM with random searched hyperparameters on par with Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [30, 40, 50, 60, 70],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_samples_split': [2, 3, 4, 5],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "GBC = GradientBoostingClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 480 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed: 14.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed: 23.7min finished\n",
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_sampl...      subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'max_depth': [30, 40, 50, 60, 70], 'max_features': [2, 3], 'min_samples_leaf': [1, 2, 3], 'min_samples_split': [2, 3, 4, 5], 'n_estimators': [100, 200, 300, 1000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(estimator = GBC, param_grid = param_grid, scoring = 'neg_mean_squared_error',\n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM - grid searched R squared: -2.0808\n",
      "0.6666666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Rank       0.72      0.74      0.73        39\n",
      "  White Belt       0.79      0.68      0.73        22\n",
      "   Blue Belt       0.50      0.80      0.62        10\n",
      " Purple Belt       0.55      0.67      0.60         9\n",
      "  Brown Belt       0.86      0.43      0.57        14\n",
      "  Black Belt       0.33      0.40      0.36         5\n",
      "\n",
      "   micro avg       0.67      0.67      0.67        99\n",
      "   macro avg       0.63      0.62      0.60        99\n",
      "weighted avg       0.70      0.67      0.67        99\n",
      "\n",
      "GBM - grid searched RMSE: 1.3143\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "#Evaluate performance of GBM with grid searched hyperparameters\n",
    "print('GBM - grid searched R squared: %.4f' % grid_search.score(X_test, y_test))\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "GBM_mse = mean_squared_error(y_pred, y_test)\n",
    "GBM_rmse = np.sqrt(forest_mse)\n",
    "print('GBM - grid searched RMSE: %.4f' % forest_rmse)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the GBM on test set: 0.657\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictors=list(X_train)\n",
    "feat_imp = pd.Series(random_model.feature_importances_, predictors).sort_values(ascending=False)\n",
    "feat_imp.plot(kind='bar', title='Importance of Features')\n",
    "plt.ylabel('Feature Importance Score')\n",
    "print('Accuracy of the GBM on test set: {:.3f}'.format(random_model.score(X_test, y_test)))\n",
    "pred=random_model.predict(X_test)\n",
    "#print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    39\n",
      "1    22\n",
      "4    14\n",
      "2    10\n",
      "3     9\n",
      "5     5\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes:  We can conclude that Age, dislike count, like count, comment count in this order are the most important features. \n"
     ]
    }
   ],
   "source": [
    "print('Notes:  We can conclude that Age, dislike count, like count, comment count in this order are the most important features. ')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
