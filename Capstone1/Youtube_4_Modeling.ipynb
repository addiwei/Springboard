{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 989 entries, 0 to 999\n",
      "Data columns (total 14 columns):\n",
      "search_word                  989 non-null object\n",
      "contentDetails.caption       989 non-null bool\n",
      "contentDetails.definition    989 non-null object\n",
      "catID                        989 non-null int64\n",
      "description                  975 non-null object\n",
      "localized.description        974 non-null object\n",
      "localized.title              989 non-null object\n",
      "tags                         930 non-null object\n",
      "title                        989 non-null object\n",
      "commentCount                 989 non-null float64\n",
      "dislikeCount                 989 non-null float64\n",
      "likeCount                    989 non-null float64\n",
      "view_bucket                  989 non-null float64\n",
      "date                         989 non-null datetime64[ns, UTC]\n",
      "dtypes: bool(1), datetime64[ns, UTC](1), float64(4), int64(1), object(7)\n",
      "memory usage: 109.1+ KB\n",
      "None\n",
      "(989, 14)\n"
     ]
    }
   ],
   "source": [
    "#Split and scale the data, train hyper parameters, Model, Evaluate\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import cross_validation, metrics  \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "%store -r df\n",
    "\n",
    "print(df.info())\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(989, 3)\n",
      "(989, 3)\n"
     ]
    }
   ],
   "source": [
    "minmax_scale = preprocessing.MinMaxScaler().fit(df[['commentCount','dislikeCount','likeCount']])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = pd.DataFrame(minmax_scale.transform(df[['commentCount','dislikeCount','likeCount']]))\n",
    "print(X.shape)\n",
    "Y = df['view_bucket']\n",
    "\n",
    "n = pd.get_dummies(df[['search_word','contentDetails.definition','contentDetails.caption']])\n",
    "\n",
    "# X = pd.concat([X, n], axis=1)\n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(989, 26)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X = pd.concat([X, n], axis=1)\n",
    "X = pd.DataFrame(np.hstack([X,n]))\n",
    "X.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(989, 27)\n"
     ]
    }
   ],
   "source": [
    "# Normalize the time series data and store as new date column\n",
    "ts = df['date']\n",
    "scaled_ts = (ts-ts.min())/(ts.max()-ts.min())\n",
    "\n",
    "X['date'] = pd.Series(scaled_ts)\n",
    "X['date'].fillna((X['date'].mean()), inplace=True)\n",
    "\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 890 entries, 304 to 684\n",
      "Data columns (total 27 columns):\n",
      "0       890 non-null object\n",
      "1       890 non-null object\n",
      "2       890 non-null object\n",
      "3       890 non-null object\n",
      "4       890 non-null object\n",
      "5       890 non-null object\n",
      "6       890 non-null object\n",
      "7       890 non-null object\n",
      "8       890 non-null object\n",
      "9       890 non-null object\n",
      "10      890 non-null object\n",
      "11      890 non-null object\n",
      "12      890 non-null object\n",
      "13      890 non-null object\n",
      "14      890 non-null object\n",
      "15      890 non-null object\n",
      "16      890 non-null object\n",
      "17      890 non-null object\n",
      "18      890 non-null object\n",
      "19      890 non-null object\n",
      "20      890 non-null object\n",
      "21      890 non-null object\n",
      "22      890 non-null object\n",
      "23      890 non-null object\n",
      "24      890 non-null object\n",
      "25      890 non-null object\n",
      "date    890 non-null float64\n",
      "dtypes: float64(1), object(26)\n",
      "memory usage: 194.7+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=.9, random_state=0)\n",
    "\n",
    "X_train.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.722\n",
      "Model:                            OLS   Adj. R-squared:                  0.713\n",
      "Method:                 Least Squares   F-statistic:                     89.54\n",
      "Date:                Wed, 19 Dec 2018   Prob (F-statistic):          5.96e-220\n",
      "Time:                        21:42:41   Log-Likelihood:                -6136.2\n",
      "No. Observations:                 890   AIC:                         1.232e+04\n",
      "Df Residuals:                     864   BIC:                         1.245e+04\n",
      "Df Model:                          25                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "0          -1771.5925    300.587     -5.894      0.000   -2361.558   -1181.627\n",
      "1           4310.9950    225.988     19.076      0.000    3867.446    4754.544\n",
      "2           2371.4207    315.554      7.515      0.000    1752.078    2990.764\n",
      "3              8.3266     24.492      0.340      0.734     -39.744      56.397\n",
      "4            -15.8665     35.480     -0.447      0.655     -85.504      53.771\n",
      "5             34.3394     36.787      0.933      0.351     -37.863     106.541\n",
      "6            -38.3425     35.890     -1.068      0.286    -108.785      32.100\n",
      "7              0.0674     36.767      0.002      0.999     -72.096      72.231\n",
      "8            219.5733     41.656      5.271      0.000     137.814     301.332\n",
      "9            -44.3046     35.609     -1.244      0.214    -114.194      25.585\n",
      "10            10.7608     36.362      0.296      0.767     -60.608      82.130\n",
      "11           -29.9639     37.139     -0.807      0.420    -102.857      42.929\n",
      "12           -17.7907     37.243     -0.478      0.633     -90.888      55.306\n",
      "13           -16.4434     36.373     -0.452      0.651     -87.832      54.946\n",
      "14            27.7905     36.323      0.765      0.444     -43.502      99.083\n",
      "15           -10.9677     36.586     -0.300      0.764     -82.776      60.841\n",
      "16           -14.9244     36.639     -0.407      0.684     -86.835      56.987\n",
      "17            30.8180     34.936      0.882      0.378     -37.751      99.387\n",
      "18           105.3818     46.510      2.266      0.024      14.097     196.667\n",
      "19            45.7970     36.409      1.258      0.209     -25.663     117.257\n",
      "20           -11.8136     35.570     -0.332      0.740     -81.628      58.001\n",
      "21            -4.7630     38.002     -0.125      0.900     -79.351      69.825\n",
      "22            10.4114     35.341      0.295      0.768     -58.953      79.776\n",
      "23           -35.1025     35.794     -0.981      0.327    -105.357      35.152\n",
      "24            87.9484     56.962      1.544      0.123     -23.852     199.749\n",
      "25           156.7084     63.111      2.483      0.013      32.840     280.576\n",
      "date         -82.9934     66.396     -1.250      0.212    -213.310      47.324\n",
      "==============================================================================\n",
      "Omnibus:                      423.292   Durbin-Watson:                   1.986\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           453642.763\n",
      "Skew:                          -0.542   Prob(JB):                         0.00\n",
      "Kurtosis:                     113.598   Cond. No.                     2.25e+15\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 3.26e-28. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = sm.OLS(list(y_train), X_train.astype(float)).fit()\n",
    "predictions = model.predict(X) \n",
    " \n",
    "print_model = model.summary()\n",
    "print(print_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression R squared: 0.7738\n",
      "Linear Regression RMSE: 323.4786\n",
      "Linear Regression MAE: 117.6162\n"
     ]
    }
   ],
   "source": [
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "print('Linear Regression R squared: %.4f' % regressor.score(X_test, y_test))\n",
    "\n",
    "lin_mse = mean_squared_error(y_pred, y_test)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print('Linear Regression RMSE: %.4f' % lin_rmse)\n",
    "\n",
    "lin_mae = mean_absolute_error(y_pred, y_test)\n",
    "print('Linear Regression MAE: %.4f' % lin_mae)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try random forest with default hyperparameters\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "forest_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest R squared: 0.2743\n",
      "Random Forest RMSE: 579.3618\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest R squared: %.4f' % forest_reg.score(X_test, y_test))\n",
    "y_pred = forest_reg.predict(X_test)\n",
    "forest_mse = mean_squared_error(y_pred, y_test)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "print('Random Forest RMSE: %.4f' % forest_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'criterion': 'mse', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': 1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and select optimal hyperparameters\n",
    "print(forest_reg.get_params())\n",
    "\n",
    "#Create a random grid for possible parameters to attempt and then use random search\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  7.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring='neg_mean_squared_error',\n",
       "          verbose=2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, scoring = 'neg_mean_squared_error', n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 50, 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "#Print and evaluate best parameters from the search\n",
    "print(rf_random.best_params_)\n",
    "\n",
    "best_random = rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate performance of the base model w/ searched hyperparameters\n",
    "\n",
    "random_model = RandomForestRegressor(n_estimators = 200, min_samples_split=2, min_samples_leaf=1, \n",
    "                                   max_features = 'sqrt', max_depth = 50, bootstrap = True, random_state = 42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=50,\n",
       "           max_features='sqrt', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest R squared: 0.7320\n",
      "Random Forest RMSE: 352.0735\n"
     ]
    }
   ],
   "source": [
    "#Evaluate performance of random forest with random searched hyperparameters\n",
    "print('Random Forest R squared: %.4f' % random_model.score(X_test, y_test))\n",
    "y_pred = random_model.predict(X_test)\n",
    "forest_mse = mean_squared_error(y_pred, y_test)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "print('Random Forest RMSE: %.4f' % forest_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Grid Search to see if we can improve the hyperparameters further. \n",
    "# Setting grid around the previously identified optimal values\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [30, 40, 50, 60, 70],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_samples_split': [2, 3, 4, 5],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, scoring = 'neg_mean_squared_error',\n",
    "                          cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 480 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   47.7s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  2.6min\n"
     ]
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate performance of random forest with grid search hyperparameters\n",
    "print('Random Forest R squared: %.4f' % grid_search.score(X_test, y_test))\n",
    "y_pred = grid_search.predict(X_test)\n",
    "forest_mse = mean_squared_error(y_pred, y_test)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "print('Random Forest RMSE: %.4f' % forest_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Gradient Boosting\n",
    "model = ensemble.GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradient Boosting R squared\": %.4f' % model.score(X_test, y_test))\n",
    "y_pred = model.predict(X_test)\n",
    "model_mse = mean_squared_error(y_pred, y_test)\n",
    "model_rmse = np.sqrt(model_mse)\n",
    "print('Gradient Boosting RMSE: %.4f' % model_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic GBM Regressor performance was poor - try tuning learning rate and number of trees\n",
    "\n",
    "p_test3 = {'learning_rate':[0.15,0.1,0.05,0.01,0.005,0.001], 'n_estimators':[100,200,300,400,500]}\n",
    "\n",
    "tuning = GridSearchCV(estimator =GradientBoostingRegressor(max_depth=4, min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10), \n",
    "            param_grid = p_test3,n_jobs=4,iid=False, cv=5)\n",
    "tuning.fit(X_train,y_train)\n",
    "tuning.grid_scores_, tuning.best_params_, tuning.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tune max_depth utilizing learning_rate of .05 and n_estimators of 300\n",
    "\n",
    "p_test2 = {'max_depth':[2,3,4,5,6,7] }\n",
    "tuning = GridSearchCV(estimator =GradientBoostingRegressor(learning_rate=0.05,n_estimators=300, min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10), \n",
    "            param_grid = p_test2,n_jobs=4,iid=False, cv=5)\n",
    "tuning.fit(X_train,y_train)\n",
    "tuning.grid_scores_, tuning.best_params_, tuning.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = GradientBoostingClassifier(learning_rate=0.05, n_estimators=300,max_depth=4, min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10)\n",
    "model1.fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors=list(X_train)\n",
    "feat_imp = pd.Series(model1.feature_importances_, predictors).sort_values(ascending=False)\n",
    "feat_imp.plot(kind='bar', title='Importance of Features')\n",
    "plt.ylabel('Feature Importance Score')\n",
    "print('Accuracy of the GBM on test set: {:.3f}'.format(model1.score(X_test, y_test)))\n",
    "pred=model1.predict(X_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try tree related parameters\n",
    "\n",
    "p_test4 = {'min_samples_split':[2,4,6,8,10,20,40,60,100], 'min_samples_leaf':[1,3,5,7,9]}\n",
    "\n",
    "tuning = GridSearchCV(estimator =GradientBoostingRegressor(learning_rate=0.05, n_estimators=300,max_depth=4, subsample=1,max_features='sqrt', random_state=10), \n",
    "            param_grid = p_test4,n_jobs=4,iid=False, cv=5)\n",
    "tuning.fit(X_train,y_train)\n",
    "tuning.grid_scores_, tuning.best_params_, tuning.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
