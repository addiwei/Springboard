{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 989 entries, 0 to 999\n",
      "Data columns (total 10 columns):\n",
      "search_word                  989 non-null object\n",
      "contentDetails.caption       989 non-null bool\n",
      "contentDetails.definition    989 non-null object\n",
      "catID                        989 non-null int64\n",
      "commentCount                 989 non-null float64\n",
      "dislikeCount                 989 non-null float64\n",
      "likeCount                    989 non-null float64\n",
      "tags_length                  989 non-null int64\n",
      "view_bucket                  989 non-null float64\n",
      "date                         989 non-null datetime64[ns, UTC]\n",
      "dtypes: bool(1), datetime64[ns, UTC](1), float64(4), int64(2), object(2)\n",
      "memory usage: 78.2+ KB\n",
      "None\n",
      "(989, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "#Split and scale the data, train hyper parameters, Model, Evaluate\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "%store -r df\n",
    "\n",
    "print(df.info())\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.preprocessing.data.MinMaxScaler'>\n",
      "(989, 4)\n",
      "(989, 23)\n",
      "(989, 27)\n",
      "(989,)\n",
      "            0            1            2         3      4  5  6  7  8  9  ...  \\\n",
      "0  0.000190146  0.000486296  0.000165565  0.446154  False  1  0  0  0  0 ...   \n",
      "1    0.0186383  4.25509e-05  1.24485e-05  0.169231  False  1  0  0  0  0 ...   \n",
      "2    0.0186383  0.000316092  0.000129998  0.169231  False  1  0  0  0  0 ...   \n",
      "3    0.0186383   0.00124005  0.000681999  0.123077  False  1  0  0  0  0 ...   \n",
      "4  0.000104011   0.00013981  0.000127686  0.215385  False  1  0  0  0  0 ...   \n",
      "\n",
      "  17 18 19 20 21 22 23 24 25 26  \n",
      "0  0  0  0  0  0  0  0  0  1  0  \n",
      "1  0  0  0  0  0  0  0  0  1  0  \n",
      "2  0  0  0  0  0  0  0  0  1  0  \n",
      "3  0  0  0  0  0  0  0  0  1  0  \n",
      "4  0  0  0  0  0  0  0  0  1  0  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "minmax_scale = preprocessing.MinMaxScaler().fit(df[['commentCount','dislikeCount','likeCount','tags_length']])\n",
    "print(type(minmax_scale))\n",
    "\n",
    "X = pd.DataFrame(minmax_scale.transform(df[['commentCount','dislikeCount','likeCount','tags_length']]))\n",
    "print(X.shape)\n",
    "Y = df['view_bucket']\n",
    "\n",
    "n = pd.get_dummies(df[['search_word','contentDetails.definition','contentDetails.caption']])\n",
    "print(n.shape)\n",
    "# X = pd.concat([X, n], axis=1)\n",
    "\n",
    "X = pd.DataFrame(np.hstack([X,n]))\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(X.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z002xczx\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=.9, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.722\n",
      "Model:                            OLS   Adj. R-squared:                  0.713\n",
      "Method:                 Least Squares   F-statistic:                     89.56\n",
      "Date:                Thu, 13 Dec 2018   Prob (F-statistic):          5.55e-220\n",
      "Time:                        11:36:46   Log-Likelihood:                -6136.1\n",
      "No. Observations:                 890   AIC:                         1.232e+04\n",
      "Df Residuals:                     864   BIC:                         1.245e+04\n",
      "Df Model:                          25                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "0          -1783.4111    300.175     -5.941      0.000   -2372.569   -1194.253\n",
      "1           4310.2497    225.978     19.074      0.000    3866.720    4753.779\n",
      "2           2385.1102    315.450      7.561      0.000    1765.972    3004.249\n",
      "3             63.1457     48.368      1.306      0.192     -31.788     158.079\n",
      "4              0.7374     25.102      0.029      0.977     -48.531      50.006\n",
      "5             -5.2695     35.396     -0.149      0.882     -74.742      64.203\n",
      "6             22.5706     36.480      0.619      0.536     -49.029      94.170\n",
      "7            -51.5417     35.662     -1.445      0.149    -121.535      18.452\n",
      "8             -6.0468     36.192     -0.167      0.867     -77.080      64.987\n",
      "9            202.8854     42.152      4.813      0.000     120.153     285.618\n",
      "10           -58.0127     34.889     -1.663      0.097    -126.489      10.464\n",
      "11            -4.1441     35.438     -0.117      0.907     -73.698      65.410\n",
      "12           -40.2774     36.164     -1.114      0.266    -111.257      30.702\n",
      "13           -31.1201     36.180     -0.860      0.390    -102.131      39.891\n",
      "14           -31.0880     35.204     -0.883      0.377    -100.183      38.007\n",
      "15            19.1091     35.528      0.538      0.591     -50.622      88.840\n",
      "16           -22.4399     35.500     -0.632      0.527     -92.117      47.237\n",
      "17           -37.7106     36.076     -1.045      0.296    -108.517      33.096\n",
      "18            34.4841     35.268      0.978      0.328     -34.736     103.705\n",
      "19            98.4106     46.566      2.113      0.035       7.015     189.806\n",
      "20            42.1297     36.135      1.166      0.244     -28.792     113.052\n",
      "21           -24.5389     34.608     -0.709      0.478     -92.464      43.386\n",
      "22           -19.0089     36.807     -0.516      0.606     -91.250      53.233\n",
      "23             6.3135     35.326      0.179      0.858     -63.022      75.649\n",
      "24           -21.3528     35.181     -0.607      0.544     -90.402      47.697\n",
      "25            -0.0608     16.747     -0.004      0.997     -32.931      32.810\n",
      "26            73.4126     33.248      2.208      0.028       8.156     138.670\n",
      "==============================================================================\n",
      "Omnibus:                      421.846   Durbin-Watson:                   1.979\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           457166.327\n",
      "Skew:                          -0.529   Prob(JB):                         0.00\n",
      "Kurtosis:                     114.027   Cond. No.                     2.25e+15\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 1.96e-28. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = sm.OLS(list(y_train), X_train.astype(float)).fit()\n",
    "predictions = model.predict(X) \n",
    " \n",
    "print_model = model.summary()\n",
    "print(print_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression R squared: 0.7743\n",
      "Linear Regression RMSE: 323.0929\n",
      "Linear Regression MAE: 116.4646\n"
     ]
    }
   ],
   "source": [
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "print('Linear Regression R squared: %.4f' % regressor.score(X_test, y_test))\n",
    "\n",
    "lin_mse = mean_squared_error(y_pred, y_test)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print('Linear Regression RMSE: %.4f' % lin_rmse)\n",
    "\n",
    "lin_mae = mean_absolute_error(y_pred, y_test)\n",
    "print('Linear Regression MAE: %.4f' % lin_mae)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try random forest with default hyperparameters\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "forest_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest R squared: 0.5693\n",
      "Random Forest RMSE: 446.3159\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest R squared: %.4f' % forest_reg.score(X_test, y_test))\n",
    "y_pred = forest_reg.predict(X_test)\n",
    "forest_mse = mean_squared_error(y_pred, y_test)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "print('Random Forest RMSE: %.4f' % forest_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'criterion': 'mse', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': 1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "# Default random forest results not impressive, so evaluate and select optimal hyperparameters\n",
    "print(forest_reg.get_params())\n",
    "\n",
    "#Create a random grid for possible parameters to attempt and then use random search\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   20.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1000, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 110, 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "#Print and evaluate best parameters from the search\n",
    "print(rf_random.best_params_)\n",
    "\n",
    "best_random = rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Average Error: 122.8202 degrees.\n",
      "Accuracy = 40.83%.\n"
     ]
    }
   ],
   "source": [
    "#Evaluate performance of the base model w/ searched hyperparameters\n",
    "\n",
    "base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
    "base_model.fit(X_train, y_train)\n",
    "predictions = base_model.predict(X_test)\n",
    "errors = abs(predictions - y_test)\n",
    "mape = 100 * np.mean(errors / y_test)\n",
    "accuracy = 100 - mape\n",
    "print('Model Performance')\n",
    "print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Average Error: 97.5241 degrees.\n",
      "Accuracy = -4.82%.\n"
     ]
    }
   ],
   "source": [
    "predictions = best_random.predict(X_test)\n",
    "errors = abs(predictions - y_test)\n",
    "mape = 100 * np.mean(errors / y_test)\n",
    "accuracy = 100 - mape\n",
    "print('Model Performance')\n",
    "print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "print('Accuracy = {:0.2f}%.'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Grid Search to see if we can improve the hyperparameters further. \n",
    "# Setting grid around the previously identified optimal values\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [30, 40, 50, 60, 70],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_samples_split': [2, 3, 4, 5],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 480 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   22.1s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   43.5s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 50,\n",
       " 'max_features': 3,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 3,\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Average Error: 105.4011 degrees.\n",
      "Accuracy = -51.24%.\n"
     ]
    }
   ],
   "source": [
    "best_grid = grid_search.best_estimator_\n",
    "predictions = best_grid.predict(X_test)\n",
    "errors = abs(predictions - y_test)\n",
    "mape = 100 * np.mean(errors / y_test)\n",
    "accuracy = 100 - mape\n",
    "print('Model Performance')\n",
    "print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "print('Accuracy = {:0.2f}%.'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try Gradient Boosting\n",
    "model = ensemble.GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting R squared\": 0.7255\n",
      "Gradient Boosting RMSE: 356.2995\n"
     ]
    }
   ],
   "source": [
    "print('Gradient Boosting R squared\": %.4f' % model.score(X_test, y_test))\n",
    "y_pred = model.predict(X_test)\n",
    "model_mse = mean_squared_error(y_pred, y_test)\n",
    "model_rmse = np.sqrt(model_mse)\n",
    "print('Gradient Boosting RMSE: %.4f' % model_rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
